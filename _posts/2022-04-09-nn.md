---
layout: post
topic: ml #st/math/ml/cs/
title:  "신경망 네트워크 Neural Networks and Deep Learning"
date:  2022-04-09 17:16:21
---
먼저 이 글은 "Hands-on Machine Learning with Scikit-Learn, Keras, and Tehnsorflow"를 바탕으로 
제 개인 공부를 위해 요약 정리한 것임을 말씀드립니다. 사진과 코드는 위의 책에서 가져온 것임을 밝혀드립니다. 
자세한 사항은 책을 참고해주세요.

# Training Deep Neural Networks
이제 부터 deep neural network에 대해서 설명할 것이다. "deep"이라는 말처럼 깊은 신경망을 이야기하는데 간단히 말해
hidden layer가 많은 artificial neural network를 말한다. 왜 이것을 쓰냐고? 기존에 ANN로 해결되지 않았던
복잡한 문제들을 풀기 위해 많은 layers를 추가함으로써 더 추상적인 정보를 학습하기 위해서 한다.

하지만, 여러 개의 layer를 이어 붙여서 생기는 문제가 아래와 같이 발생했다.
- vanishing / exloding gradients problems.
- not enough training data
- training slow
- overfitting

그렇다면 이런 것들을 해결하기 위해 어떤 연구를 지금까지 해왔을까? 그것을 탐험해보도록 하자.

## The Vanishing/Exploding Gradients Problems
### Glorot and He Initialization
### Nonsaturating Activation Functions
### Batch Normalization
### Gradient Clipping

## Reusing Pretrained Layers
### Transfer Learning with Keras
### Unsupervised Pretraining
### Pretraining on an Auxiliary Task

## Faster Optimizers
### Momentum Optimization
### Nesterov Accelerated Gradient
### AdaGrad
### RMSProp
### Adam and Nadam Optimization
### Learning Rate Scheduling

## Avoiding Overfitting Through Regularization
### ℓ1 and ℓ2 Regularization
### Dropout
### Monte Carlo (MC) Dropout
### Max-Norm Regularization
### Summary and Practical Guidelines